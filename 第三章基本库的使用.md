# urlib

* request：发送请求，给库传入URL以及额外的参数。

* error：异常处理。在出现异常时，捕获异常，并进行其他操作。

* parse：一个工具模块，提供URL处理方法，如拆分、解析、合并等。

* robotparse：用于识别网站的robots.txt文件，判断网站是否可爬，使用较少。

### 发送请求 request

``` 
    import urlib.request
    
    response = urlib.request.urlopen("https://www.python.org/")
    print(response.read().decode("utf-8"))
```
> 注：原书中的网址在最后少了“/”，已在此补全

urlopen()的API
```
urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, cadefault=False, context=None)
```

#### data 参数
它以bytes的方式提供内容，并且使用data参数后，请求方式将变为post
```
import urllib.parse
import urllib.request
data = bytes(urllib.parse.urlencode({'word':'hello'}), encoding='utf8')
response= urllib.request.urlopen('http://httpbin.org/post', data=data)
print(response.read().decode("utf-8"))
```
> 注：原书在print里的内容未加.decode(utf-8)，导致print的内容没有换行，此处已补全

运行结果如下：
```
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "word": "hello"
  }, 
  "headers": {
    "Accept-Encoding": "identity", 
    "Content-Length": "10", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "httpbin.org", 
    "User-Agent": "Python-urllib/3.8", 
    "X-Amzn-Trace-Id": "Root=1-60de0cda-04270d3463fc0f70377e9533"
  }, 
  "json": null, 
  "origin": "64.251.146.204", 
  "url": "http://httpbin.org/post"
}
```
可以看到我们这里得到了表单。其中在“form”处包含参数“word”：“hello”。
这里最大的特点就是在请求里传输了data，而这个特点有在响应表单里得到了响应（这个网站对传输数据有特别的响应）。
但这里data的实际作用我仍然未见到更实际的实例。

#### timeout 参数
该参数用于设置超时时间，单位为秒，意为超过这个设置时间还未得到响应，则抛出异常。
```
import urllib.request

response = urllib.request.urlopen("http://httpbin.org/get", timeout=0.01)
print(response.read().decode("utf-8"))
```
> 本书首先与上一个例子一直，缺少decode。另外本书采用的是1s的timeout，我这里运行太快，导致根本不报错，因此这里我采用的是0.01s

运行结果如下：
```
Traceback (most recent call last):
  File "C:\Users\Zheng\Anaconda3\envs\pythonProject\lib\urllib\request.py", line 1350, in do_open
    h.request(req.get_method(), req.selector, req.data, headers,
  File "C:\Users\Zheng\Anaconda3\envs\pythonProject\lib\http\client.py", line 1255, in request
    self._send_request(method, url, body, headers, encode_chunked)
...
urllib.error.URLError: <urlopen error timed out>
```
这里可以看到URLError抛出了时间异常，因此可以使用该方法配合try来跳过某些反应过慢的网站的抓取。

Try Ex：
```
import socket
import urllib.request
import urllib.error

try:
    response = urllib.request.urlopen("http://httpbin.org/get", timeout=0.01)
except urllib.error.URLError as e:
    if isinstance(e.reason, socket.timeout):
        print("TIME OUT")
```
在这里socket.timeout表示超时报错

#### 其他参数

context - ssl.SSLContext类型，指定SSL设置。

cafile - CA证书

capath - 指明路径

以上为简单的请求以及网页抓取

### request

urlopen()实现了请求，但几个简单参数不足以构建完整请求，此时我们可以采用Request类，加入Headers等信息。

Ex：
```
import urllib.request

request = urllib.request.Request("https://www.python.org/")
response = urllib.request.urlopen(request)
print(response.read().decode("utf-8"))
```
> 注：原书中的网址给错了，少了www。不过有意思的是他最后加了decode，可见之前没注意到，现在注意到了。

这个例子与之前最大的区别就是从url链接变为了一个Request对象。但返回的内容并没有什么改变。

Request的API：
```
class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, univerifiable=False, method=None)
参数顺序： url 网址，data 数据（bytes类型），headers 请求头（字典，可以采用add_header()来添加，通常修改User-Agent来伪装为浏览器），
          origin_req_host 请求方的host或者IP，unverifiable 这个请求是否无法验证，意为用户没有权限去做某件事情（例如：自动抓取图像的权限，此时值为True）
          mothod 字符串，指示请求使用的方法。
```

ex:
```
from urllib import request, parse

url = "http://httpbin.org/post"

dict = {
    "name": "Germey"
}
data = bytes(parse.urlencode(dict), encoding="utf8")

headers = {
    "User-Agent": "Mozilla/4.0 (compatible; MSIE 5.5; Windows NT",
    "Host": "httpbin.org"
}

req = request.Request(url=url, data=data, headers=headers, method="POST")
response = request.urlopen(req)
print(response.read().decode("utf-8"))
```
结果如下：
```
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "name": "Germey"
  }, 
  "headers": {
    "Accept-Encoding": "identity", 
    "Content-Length": "11", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "httpbin.org", 
    "User-Agent": "Mozilla/4.0 (compatible; MSIE 5.5; Windows NT", 
    "X-Amzn-Trace-Id": "Root=1-60de21b6-42b068e50324cf435ff893c2"
  }, 
  "json": null, 
  "origin": "64.251.146.204", 
  "url": "http://httpbin.org/post"
}
```
说明data、header、method都被更改了。

另外，headers也可以也可以根据req.add_header("User-Agent", "Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)')的方式更改。

### 高级用法

用于处理更高级的操作，例如cookies，代理设置等。这时候就需要用到Headler了。

BaseHandler类具有一些基本方法，例如default_open()、protocol_request()等。

各个Handler子类：

- HTTPDefaultErrorHandler：用于处理HTTP响应错误。

- HTTPRedirectHandler：用于重定向

- HTTPCookieProcessor：用于处理Cookies

- ProxyHanddler：用于设置代理

- HTTPPasswordMgr：用于管理密码

- HTTPBasicAuthHandler：用于管理认证，当一个链接需要认证时，它可以解决认证问题。

另一个重要的类是Opener。Opener是借由Headler完成的。

#### 密码

Ex HTTPBasicAuthHandler:
```
from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener
from urllib.error import URLError

username = "username"
password = "password"
url = "http://localhost:5000/"

p = HTTPPasswordMgrWithDefaultRealm()
p.add_password(None, url, username, password)
auth_handler = HTTPBasicAuthHandler(p)
opener = build_opener(auth_handler)

try:
    result = opener.open(url)
    html = result.read().decode("utf-8")
    print(html)
except URLError as e:
    print(e.reason)
```
> 注：这里的网站已经打不开了，所以不具备实际运行能力，网站原本运行的样子：
![image](https://user-images.githubusercontent.com/77183284/124187278-36ce6900-da83-11eb-84c1-72d1890b5499.png)

这里值得关注的重点是密码传输，这个过程是通过构建HTTPPasswordMgrWithDefaultRealm对象，并给对象添加password信息，
在将其用HTTPBasicAuthHandler包装起来后，通过build_opener()打开完成。

#### 代理

Ex:
```
from urllib.error import URLError
from urllib.request import ProxyHandler, build_opener

proxy_handler = ProxyHandler({
    "http": "http://www.kproxy.com",
    "https": "https://www.kproxy.com"
})
opener = build_opener(proxy_handler)
try:
    response = opener.open("http://www.baidu.com")
    print(response.read().decode("utf-8", 'ignore'))
except URLError as e:
    print(e.reason)
```
> 注：原书的代理网站崩了，我这里重新在网上找了一个免费的网址，另外decode部分有错误，我这里选择了ignore

这里的重点自然是我们找到了一种代理方式来获取我们想要的信息。

#### Cookies

Ex:
```
import http.cookiejar, urllib.request

# cookie = http.cookiejar.CookieJar()
# cookie_handler = urllib.request.HTTPCookieProcessor(cookie)
# opener = urllib.request.build_opener(cookie_handler)
# response = opener.open("http://www.baidu.com")
# for item in cookie:
#     print(item.name + "=" + item.value)

# 也可以使用文件保存

# filename = "Chapter_3-cookies.txt"
# cookie = http.cookiejar.MozillaCookieJar(filename)
# cookie_headler = urllib.request.HTTPCookieProcessor(cookie)
# opener = urllib.request.build_opener(cookie_headler)
# response = opener.open("http://www.baidu.com")
# cookie.save(ignore_discard=True, ignore_expires=True)
# print(response.read().decode("utf-8"))

# 将之前保存的cookies读取
cookie = http.cookiejar.MozillaCookieJar()
cookie.load("Chapter_3-cookies.txt", ignore_discard=True, ignore_expires=True)
headler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(headler)
response = opener.open("https://baidu.com")
print(response.read().decode("utf-8"))
```
对比目前所学，我发现在opener.open(headler)之前的部分是重点，分别对应了不同的情况，例如提供密码，代理，以及cookies。那么问题来了，
如果遇到混杂式需求，我应该怎么做呢？因为对应的构建healder方法以及提供的信息是不同的。

具体流程为：初始化headler信息 -> 对应的headler信息与方法来构建headler-> build_open()创建opener -> opener打开网址 -> response接受信息

### 异常处理

#### URLError
URLError类来自urllib，继承OSError类，可以处理request模块生成的异常。

Ex:
```
from urllib import request, error
try:
    response = request.urlopen("https://cuiqingcai.com/index.htm")
except error.URLError as e:
    print(e.reason)
```

#### HTTPError

用于处理HTTP请求错误，它有三个属性code（返回状态码），reason（返回错误的原因）和headers（返回请求头）

Ex:
```
from urllib import request, error
try:
    response = request.urlopen("https://cuiqingcai.com/index.htm")
except error.HTTPError as e:
    print(e.reason, e.code, e.headers, sep='\n')
except error.URLError as e:
    print(e.reason)
else:
    print("Request Successfully")
```
reason也可能是个对象

Ex:
```
import socket
import urllib.request
import urllib.error

try:
    reaponse = urllib.request.urlopen("https://www.baidu.com", timeout=0.011)
except urllib.error.URLError as e:
    print(type(e.reason))
    if isinstance(e.reason, socket.timeout):
        print("TIME OUT")
```

### 解析链接

#### urlparse()

可以用于URL的识别与分段

Ex:
```
from urllib.parse import urlparse
result = urlparse("https://www.baidu.com/index.html;user?id=5#comment")
print(type(result), result)
```
结果如下：
```
<class 'urllib.parse.ParseResult'> ParseResult(scheme='https', netloc='www.baidu.com', path='/index.html', params='user', query='id=5', fragment='comment')
```
> scheme表示协议，netloc表示域名，path为访问路径，params代表参数，query为查询条件，get类型的URL，#后面表示锚点，用于定位内部下拉位置

urlparse()的API
```
urllib.parse.urlparse(urlstring, scheme='', allow_fragments=True)
```
> urlstring: 待解析的URL；scheme：默认协议，在链接未带协议的情况下，默认的协议；allow_fragments：是否忽略fragment，false->忽略，fragment会被解析为path、parameter、query的一部分

#### urlunparse()

之前的反向操作

Ex:
```
from urllib.parse import urlunparse

data = ["http", "www.baidu.com", "index.html", "user", "a=6", "comment"]
print(urlunparse(data))
```

#### urlsplit()、urlunsplit()、urljoin()

urlsplit()、urlupsplit()与前面的方法很相似，不过拆分和组合变成了5个，params合并到了path中。urljoin()则有两个参数分别是base_url和新的链接，这个参数会在新的链接参数缺少scheme、netloc和path时，根据基础链接对新的链接部分进行补充。

#### urlencode()

GET请求参数的时候有用，将一些字典信息转变为URL

Ex:
```
from urllib.parse import urlencode

params = {
    "name": "germey",
    "age": 22
}
base_url = "http://www.baidu.com"
url = base_url + urlencode(params)
print(url)
```

结果如下：
```
http://www.baidu.comname=germey&age=22
```

#### parse_qs()

将URL信息转变为字典信息

Ex:
```
from urllib.parse import parse_qs

query = "name=germey&age=22"
print(parse_qs(query))
```

结果如下：
```
{'name': ['germey'], 'age': ['22']}
```

#### parse_qsl()

将参数转化为元组组成的列表

```
from urllib.parse import parse_qsl

query = "name=qermey&age=22"
print(parse_qsl(query))
```

结果如下：
```
[('name', 'qermey'), ('age', '22')]
```
#### quote()

将内容转化为URL编码的格式。URL带中文参数的时候，可能会导致乱码的问题。

Ex:
```
# Example 17 parse_qsl()
from urllib.parse import quote

keyword = "壁纸"
url = "https://www.baidu.com/s?wd=" + quote(keyword)
print(url)
```

#### unquote()

将URL解码

Ex:
```
from urllib.parse import unquote

url = "http://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8"
print(unquote(url))
```

至此，通过上面的代码可以实现对网站链接的拆解、分析与组装。

### 分析Robots协议

#### Robots协议

Robots协议也称爬虫协议、机器人协议，全名叫网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索那些页面可以抓取，哪些不可以。通常叫做robots.txt文本。

![image](https://user-images.githubusercontent.com/77183284/124341015-0b30a900-db7f-11eb-9296-2dea4b4f6844.png)

#### robotparser

RobotFileParser类可以根据网站的robots.txt来判断爬虫是否有权限爬取网页。

声明：urllib.robotparser.RobotFileParser(url="")

类的内置方法：
|方法|作用|
|:-:|:-:|
|set_url()|用来设置robots.txt文件的链接|
|read()|读取robots.txt文件并且分析|
|parse()|用来解析robots.txt文件|
|can_fetch()|传入两个参数，一个时User-agent，另一个要抓取的URL。返回该引擎搜索是否可以抓取这个内容|
|mtim()|返回上次抓取和分析robots.txt的时间，对于长时间的分析和抓取的搜索爬虫很有必要|
|modified()|将当前时间设置为上次抓取和分析robots.txt的时间|

Ex:
```
from urllib.robotparser import RobotFileParser
from urllib.request import urlopen

rp = RobotFileParser()
rp.parse(urlopen("http://www.baidu.com/robots.txt").read().decode("utf-8").split("\n"))
print(rp.can_fetch("*", "https://www.baidu.com/s?cl=3&tn=baidutop10&fr=top1000&wd=%E5%8D%AB%E5%9B%BD%E6%88%8D%E8%BE%B9%E8%8B%B1%E9%9B%84%E9%81%97%E7%89%A9%E9%A6%96%E6%AC%A1%E5%9C%A8%E5%86%9B%E5%8D%9A%E5%B1%95%E5%87%BA&rsv_idx=2&rsv_dl=fyb_n_homepage&hisfilter=1"))
print(rp.can_fetch("*", "https://www.baidu.com/s?cl=3&tn=baidutop10&fr=top1000&wd=%E6%97%A5%E6%9C%AC%E7%A6%8F%E5%B2%9B%E5%87%BA%E7%8E%B0%E6%94%BE%E5%B0%84%E6%80%A7%E6%9D%82%E4%BA%A4%E9%87%8E%E7%8C%AA&rsv_idx=2&rsv_dl=fyb_n_homepage&hisfilter=1"))
```
> 注：这里我修改了网站，然后运行成功了。这里的网站有一点需要注意，就是它允许一部分爬虫，这就要考虑更改爬虫名字。

# 使用request

### 基本用法

Ex:
```
import requests

r = requests.get("https://www.baidu.com/")
print(type(r))
print(r.status_code)
print(type(r.text))
print(r.text)
print(r.cookies)
```

调用get()方法与urlopen()相同的操作，得到了Respond对象。除此以外，还可以将其他请求用一句话完成：
```
r = requests.post("http://httpbin.org/post")
r = requests.put("http://httpbin.org/post")
r = requests.delete("http://httpbin.org/post")
r = requests.head("http://httpbin.org/post")
r = requests.options("http://httpbin.org/post")
```
### get
Ex:
```
import requests

r = requests.get("http://httpbin.org/get")
print(r.text)
```

带参数Ex:
```
import requests

data = {
    "name": "germey",
    "age": 22
}
r = requests.get("http://httpbin.org/get", params=data)
print(r.text)
```

网页的返回类型是str类型，并且是JSON格式。想要解析返回结果，得到字典格式，则要调用json()方法。

Ex:
```
import requests

r = requests.get("http://httpbin.org/get")
print(type(r.text))
print(r.json())
print(type(r.json()))
```
> 注：如果返回结果不是JSON格式，则会出现解析错误，抛出json.decoder.JSONDecodeError异常

#### 抓取网站

Ex 抓取知乎发现页面:
```
import requests
import re

headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mas OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) \
    Chrome/52.0.2743.116 Safari/537.36"
}
r = requests.get("https://www.zhihu.com/explore", headers=headers)
print(r.text)
pattern = re.compile("explore-feed.*?qustion_link.*?>(.*?)</a>", re.S)
titles = re.findall(pattern, r.text)
print(titles)
```
> 注：由于知乎版本更新，这里已经爬取不到数据了。由于它采用的是正则表达式，暂时我还没想好如何修改此处。

#### 抓取二进制数据

这里抓取的页面为HTML格式，如果要抓去图片、音频、视频等文件，则需要找到它们的二进制码。

Ex 抓取GitHub的小图标:
```
import requests

r = requests.get("https://github.com/favicon.ico")
print(r.text)
print(r.content)
```

Ex 将上面抓取的图标保存下来:
```
import requests

r = requests.get("https://github.com/favicon.ico")
with open("Chapter_3_favicon.ico", "wb") as f:
    f.write(r.content)
```
> 书看到这里，我总结到两个比较重要的事情，一个就是请求的表头、内容什么需要是什么样子，主要需要注意的是不要被反爬虫拦住。
> 另一个比较重要的事情是，网站响应里，我如何能找到自己想要的内容。

#### 添加headers

Ex:
```
import requests

headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mas OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) \
    Chrome/52.0.2743.116 Safari/537.36"
}
r = requests.get("https://www.zhihu.com/explore", headers=headers)
print(r.text)
```
> 注：在这里如果删除表头则无法爬取网站内容。

#### POST请求

Ex post请求:
```
import requests

data = {"name": "germey",
        "age": "22"
        }
r = requests.post("http://httpbin.org/post", data=data)
print(r.text)
```

结果如下：
```
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "age": "22", 
    "name": "germey"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "18", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.25.0", 
    "X-Amzn-Trace-Id": "Root=1-60e09e55-413aa763193070ad6a6f1c23"
  }, 
  "json": null, 
  "origin": "64.251.146.204", 
  "url": "http://httpbin.org/post"
}
```

### 响应

还有很多其他方法可以用来获取信息。

Ex:
```
import requests

r = requests.get("http://www.baidu.com")
print(r.status_code)
if not r.status_code == requests.codes["ok"]:
    exit()
else:
    print("Request Successfully")
print(type(r.status_code), r.status_code)
print(type(r.headers), r.headers)
print(type(r.cookies), r.cookies)
print(type(r.url), r.url)
print(type(r.history), r.history)
```
> 注：这里我将网址更换为了百度。

这里还有更多状态码

![image](https://user-images.githubusercontent.com/77183284/124362648-e3cbf180-dbfb-11eb-882f-89b353098d50.png)
![image](https://user-images.githubusercontent.com/77183284/124362654-ed555980-dbfb-11eb-9c2b-aabc56fb9704.png)
![image](https://user-images.githubusercontent.com/77183284/124362658-f34b3a80-dbfb-11eb-83db-297343d52145.png)

### 高级用法

#### 文件上传

Ex:
```
import requests

files = {"file": open("Chapter_3_favicon.ico", "rb")}
r = requests.post("http://httpbin.org/post", files =files)
print(r.text)
```
> 这里得到了一个非常长的一个结果。

#### Cookies

Ex:
```
import requests

headers = {
"Cookie": , # 这里的cookie我就不放了因为是个人账户
    "Host": "www.zhihu.com",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}
r = requests.get("https://www.zhihu.com/", headers=headers)
print(r.text)
```
> 运行结果非常好，结果非常正常的就出来了。

Ex 上一个例子的另一种写法:
```
import  requests

cookies = "这里补充cookies”
jar = requests.cookies.RequestsCookieJar()
headers = {
    "Host": "www.zhihu.com",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}
for cookies in cookies.split(";"):
    key, value = cookies.split("=", 1)
    jar.set(key,value)
r = requests.get("http://zhihu.com", cookies=jar, headers=headers)
print(r.text)
```

#### 会话维持

request中除了使用cookies以外，更方便的维持会话（不用总是复制cookies的信息）

Ex:
```
import requests

# 这里相当于有两个会话（request.get）
requests.get("http://httpbin.org/cookies/set/number/123456789")
r = requests.get("http://httpbin.org/cookies")
print(r.text)

# 这里只有一个会话s
s = requests.Session()
s.get("http://httpbin.org/cookies/set/number/123456789")
r = s.get("http://httpbin.org/cookies")
print(r.text)
```

#### SSL证书验证

request还提供证书验证功能。

Ex:

```
import requests
from requests.packages import urllib3

# 在不检测证书的时候不预警
urllib3.disable_warnings()
response = requests.get("https://www.12306.cn", verify=False)
print(response.status_code)
```
> 这里网站改版了，直接就能验证，不过这里我还是跳过了验证部分。另外这里有了一个不预警的方法，不过我还是建议预警一下。

#### 代理设置

当我们开始大规模爬取，频繁请求的时候，就可能被网站弹出验证码，跳转登陆界面，甚至是直接客户端IP封禁。这时候就会需要proxy了。

Ex:
```
import requests

proxies = {
    "http": "http://www.kproxy.com/"
}

response = requests.get("https://www.taobao.com", proxies=proxies)

# # HTTP Basic Auth 代理
# proxies = {
#      "http": "http://user:password@10.10.1.10:3128"
# }
# requests.get("http://www.taobao.com", proxies=proxies)

proxies = {
    "http": "socks5://user:password@host:port",
    "http": "socks5://user:password@host:port"
}
requests.get("https://www.taobao.com", proxies=proxies)
```
> 这里分三段，前面和后面分别是代理和sock协议代理，都成功了。中间的是HTTP Basic Auth代理，给的网站有问题，目前我还不知道HTTP Basic Auth协议的代理，所以没有改这里。

#### 超时错误 timeout

添加timeout函数来抛出超时异常。

#### 身份认证

Ex:
```
import requests
from requests.auth import HTTPBasicAuth

r = requests.get("http://localhost:5000", auth=("username", "password"))
print(r.status_code)
```
> 注：这里网站有问题，我也没有合适的替代网站，所以这段代码跑不了。

#### Prepared Request

将请求表示为数据结构，各个参数通过Requests来表示。这个数据结构叫做Prepared Request。

Ex:
```
from requests import Request, Session

url = "http://httpbin.org/post"
data = {
    "name":"germey"
}
headers = {
    "User-Agent":  "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}
s = Session()
req = Request("POST", url, data=data, headers=headers)
prepped = s.prepare_request(req)
r = s.send(prepped)
print(r.text)
```
> Request这里作为对象，在以后可以构建这个对象的队列，在调度的时候会很方便。

# 正则表达式


```




