# urlib

* request：发送请求，给库传入URL以及额外的参数。

* error：异常处理。在出现异常时，捕获异常，并进行其他操作。

* parse：一个工具模块，提供URL处理方法，如拆分、解析、合并等。

* robotparse：用于识别网站的robots.txt文件，判断网站是否可爬，使用较少。

### 发送请求 request

``` 
    import urlib.request
    
    response = urlib.request.urlopen("https://www.python.org/")
    print(response.read().decode("utf-8"))
```
> 注：原书中的网址在最后少了“/”，已在此补全

urlopen()的API
```
urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, cadefault=False, context=None)
```

#### data 参数
它以bytes的方式提供内容，并且使用data参数后，请求方式将变为post
```
import urllib.parse
import urllib.request
data = bytes(urllib.parse.urlencode({'word':'hello'}), encoding='utf8')
response= urllib.request.urlopen('http://httpbin.org/post', data=data)
print(response.read().decode("utf-8"))
```
> 注：原书在print里的内容未加.decode(utf-8)，导致print的内容没有换行，此处已补全

运行结果如下：
```
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "word": "hello"
  }, 
  "headers": {
    "Accept-Encoding": "identity", 
    "Content-Length": "10", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "httpbin.org", 
    "User-Agent": "Python-urllib/3.8", 
    "X-Amzn-Trace-Id": "Root=1-60de0cda-04270d3463fc0f70377e9533"
  }, 
  "json": null, 
  "origin": "64.251.146.204", 
  "url": "http://httpbin.org/post"
}
```
可以看到我们这里得到了表单。其中在“form”处包含参数“word”：“hello”。
这里最大的特点就是在请求里传输了data，而这个特点有在响应表单里得到了响应（这个网站对传输数据有特别的响应）。
但这里data的实际作用我仍然未见到更实际的实例。

#### timeout 参数
该参数用于设置超时时间，单位为秒，意为超过这个设置时间还未得到响应，则抛出异常。
```
import urllib.request

response = urllib.request.urlopen("http://httpbin.org/get", timeout=0.01)
print(response.read().decode("utf-8"))
```
> 本书首先与上一个例子一直，缺少decode。另外本书采用的是1s的timeout，我这里运行太快，导致根本不报错，因此这里我采用的是0.01s

运行结果如下：
```
Traceback (most recent call last):
  File "C:\Users\Zheng\Anaconda3\envs\pythonProject\lib\urllib\request.py", line 1350, in do_open
    h.request(req.get_method(), req.selector, req.data, headers,
  File "C:\Users\Zheng\Anaconda3\envs\pythonProject\lib\http\client.py", line 1255, in request
    self._send_request(method, url, body, headers, encode_chunked)
...
urllib.error.URLError: <urlopen error timed out>
```
这里可以看到URLError抛出了时间异常，因此可以使用该方法配合try来跳过某些反应过慢的网站的抓取。

Try Ex：
```
import socket
import urllib.request
import urllib.error

try:
    response = urllib.request.urlopen("http://httpbin.org/get", timeout=0.01)
except urllib.error.URLError as e:
    if isinstance(e.reason, socket.timeout):
        print("TIME OUT")
```
在这里socket.timeout表示超时报错

#### 其他参数

context - ssl.SSLContext类型，指定SSL设置。

cafile - CA证书

capath - 指明路径

以上为简单的请求以及网页抓取

### request

urlopen()实现了请求，但几个简单参数不足以构建完整请求，此时我们可以采用Request类，加入Headers等信息。

Ex：
```
import urllib.request

request = urllib.request.Request("https://www.python.org/")
response = urllib.request.urlopen(request)
print(response.read().decode("utf-8"))
```
> 注：原书中的网址给错了，少了www。不过有意思的是他最后加了decode，可见之前没注意到，现在注意到了。

这个例子与之前最大的区别就是从url链接变为了一个Request对象。但返回的内容并没有什么改变。

Request的API：
```
class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, univerifiable=False, method=None)
参数顺序： url 网址，data 数据（bytes类型），headers 请求头（字典，可以采用add_header()来添加，通常修改User-Agent来伪装为浏览器），
          origin_req_host 请求方的host或者IP，unverifiable 这个请求是否无法验证，意为用户没有权限去做某件事情（例如：自动抓取图像的权限，此时值为True）
          mothod 字符串，指示请求使用的方法。
```

ex:
```
from urllib import request, parse

url = "http://httpbin.org/post"

dict = {
    "name": "Germey"
}
data = bytes(parse.urlencode(dict), encoding="utf8")

headers = {
    "User-Agent": "Mozilla/4.0 (compatible; MSIE 5.5; Windows NT",
    "Host": "httpbin.org"
}

req = request.Request(url=url, data=data, headers=headers, method="POST")
response = request.urlopen(req)
print(response.read().decode("utf-8"))
```
结果如下：
```
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "name": "Germey"
  }, 
  "headers": {
    "Accept-Encoding": "identity", 
    "Content-Length": "11", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "httpbin.org", 
    "User-Agent": "Mozilla/4.0 (compatible; MSIE 5.5; Windows NT", 
    "X-Amzn-Trace-Id": "Root=1-60de21b6-42b068e50324cf435ff893c2"
  }, 
  "json": null, 
  "origin": "64.251.146.204", 
  "url": "http://httpbin.org/post"
}
```
说明data、header、method都被更改了。

另外，headers也可以也可以根据req.add_header("User-Agent", "Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)')的方式更改。

### 高级用法

用于处理更高级的操作，例如cookies，代理设置等。这时候就需要用到Headler了。

BaseHandler类具有一些基本方法，例如default_open()、protocol_request()等。

各个Handler子类：

- HTTPDefaultErrorHandler：用于处理HTTP响应错误。

- HTTPRedirectHandler：用于重定向

- HTTPCookieProcessor：用于处理Cookies

- ProxyHanddler：用于设置代理

- HTTPPasswordMgr：用于管理密码

- HTTPBasicAuthHandler：用于管理认证，当一个链接需要认证时，它可以解决认证问题。

另一个重要的类是Opener。Opener是借由Headler完成的。

#### 密码

Ex HTTPBasicAuthHandler:
```
from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener
from urllib.error import URLError

username = "username"
password = "password"
url = "http://localhost:5000/"

p = HTTPPasswordMgrWithDefaultRealm()
p.add_password(None, url, username, password)
auth_handler = HTTPBasicAuthHandler(p)
opener = build_opener(auth_handler)

try:
    result = opener.open(url)
    html = result.read().decode("utf-8")
    print(html)
except URLError as e:
    print(e.reason)
```
> 注：这里的网站已经打不开了，所以不具备实际运行能力，网站原本运行的样子：
![image](https://user-images.githubusercontent.com/77183284/124187278-36ce6900-da83-11eb-84c1-72d1890b5499.png)

这里值得关注的重点是密码传输，这个过程是通过构建HTTPPasswordMgrWithDefaultRealm对象，并给对象添加password信息，
在将其用HTTPBasicAuthHandler包装起来后，通过build_opener()打开完成。

#### 代理

Ex:
```
from urllib.error import URLError
from urllib.request import ProxyHandler, build_opener

proxy_handler = ProxyHandler({
    "http": "http://www.kproxy.com",
    "https": "https://www.kproxy.com"
})
opener = build_opener(proxy_handler)
try:
    response = opener.open("http://www.baidu.com")
    print(response.read().decode("utf-8", 'ignore'))
except URLError as e:
    print(e.reason)
```
> 注：原书的代理网站崩了，我这里重新在网上找了一个免费的网址，另外decode部分有错误，我这里选择了ignore

这里的重点自然是我们找到了一种代理方式来获取我们想要的信息。

#### Cookies

Ex:
```
import http.cookiejar, urllib.request

# cookie = http.cookiejar.CookieJar()
# cookie_handler = urllib.request.HTTPCookieProcessor(cookie)
# opener = urllib.request.build_opener(cookie_handler)
# response = opener.open("http://www.baidu.com")
# for item in cookie:
#     print(item.name + "=" + item.value)

# 也可以使用文件保存

# filename = "Chapter_2-cookies.txt"
# cookie = http.cookiejar.MozillaCookieJar(filename)
# cookie_headler = urllib.request.HTTPCookieProcessor(cookie)
# opener = urllib.request.build_opener(cookie_headler)
# response = opener.open("http://www.baidu.com")
# cookie.save(ignore_discard=True, ignore_expires=True)
# print(response.read().decode("utf-8"))

# 将之前保存的cookies读取
cookie = http.cookiejar.MozillaCookieJar()
cookie.load("Chapter_2-cookies.txt", ignore_discard=True, ignore_expires=True)
headler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(headler)
response = opener.open("https://baidu.com")
print(response.read().decode("utf-8"))
```
对比目前所学，我发现在opener.open(headler)之前的部分是重点，分别对应了不同的情况，例如提供密码，代理，以及cookies。那么问题来了，
如果遇到混杂式需求，我应该怎么做呢？因为对应的构建healder方法以及提供的信息是不同的。

具体流程为：初始化headler信息 -> 对应的headler信息与方法来构建headler-> build_open()创建opener -> opener打开网址 -> response接受信息
